=== Training started ===
Output dir: checkpoints/llama32-1b-lora-finetome
Batch size: 4
Grad accumulate: 4
Epochs: 2
LR: 0.0002
BF16: True
Optimizer: OptimizerNames.ADAMW_TORCH
[step 50] loss=1.2990 lr=0.000200 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1382.0MB | elapsed=1.3min, steps/s=0.64
[step 100] loss=0.9647 lr=0.000200 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1389.0MB | elapsed=2.6min, steps/s=0.63
[step 150] loss=0.9515 lr=0.000200 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1395.2MB | elapsed=3.9min, steps/s=0.64
[step 200] loss=0.9410 lr=0.000200 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1401.1MB | elapsed=5.3min, steps/s=0.63
[step 250] loss=0.8898 lr=0.000199 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1406.0MB | elapsed=6.6min, steps/s=0.63
Checkpoint saved at step 250 → checkpoints/llama32-1b-lora-finetome
[step 300] loss=0.9140 lr=0.000199 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1447.9MB | elapsed=7.9min, steps/s=0.63
[step 350] loss=0.9189 lr=0.000198 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1451.9MB | elapsed=9.2min, steps/s=0.63
[step 400] loss=0.9189 lr=0.000197 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1455.6MB | elapsed=11.2min, steps/s=0.60
[step 450] loss=0.9226 lr=0.000197 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1458.9MB | elapsed=14.0min, steps/s=0.54
[step 500] loss=0.8958 lr=0.000196 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1461.8MB | elapsed=16.8min, steps/s=0.50
Checkpoint saved at step 500 → checkpoints/llama32-1b-lora-finetome
[step 550] loss=0.8686 lr=0.000195 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1437.0MB | elapsed=19.8min, steps/s=0.46
[step 600] loss=0.8996 lr=0.000194 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1439.3MB | elapsed=22.7min, steps/s=0.44
[step 650] loss=0.8955 lr=0.000193 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1442.9MB | elapsed=24.6min, steps/s=0.44
[step 700] loss=0.9229 lr=0.000191 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1444.9MB | elapsed=25.9min, steps/s=0.45
[step 750] loss=0.8900 lr=0.000190 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1446.8MB | elapsed=27.2min, steps/s=0.46
Checkpoint saved at step 750 → checkpoints/llama32-1b-lora-finetome
[step 800] loss=0.9260 lr=0.000189 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1445.1MB | elapsed=28.6min, steps/s=0.47
[step 850] loss=0.8921 lr=0.000187 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1446.7MB | elapsed=29.9min, steps/s=0.47
[step 900] loss=0.9053 lr=0.000186 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1448.3MB | elapsed=31.2min, steps/s=0.48
[step 950] loss=0.8869 lr=0.000184 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1449.6MB | elapsed=32.5min, steps/s=0.49
[step 1000] loss=0.8610 lr=0.000182 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1450.7MB | elapsed=33.8min, steps/s=0.49
Checkpoint saved at step 1000 → checkpoints/llama32-1b-lora-finetome
[step 1050] loss=0.8396 lr=0.000180 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1452.4MB | elapsed=35.2min, steps/s=0.50
[step 1100] loss=0.8947 lr=0.000178 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1453.4MB | elapsed=36.5min, steps/s=0.50
[step 1150] loss=0.8731 lr=0.000176 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1454.5MB | elapsed=37.8min, steps/s=0.51
[step 1200] loss=0.8579 lr=0.000174 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1455.4MB | elapsed=39.2min, steps/s=0.51
[step 1250] loss=0.8708 lr=0.000172 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1456.1MB | elapsed=40.5min, steps/s=0.51
Checkpoint saved at step 1250 → checkpoints/llama32-1b-lora-finetome
[step 1300] loss=0.8825 lr=0.000170 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1457.2MB | elapsed=41.8min, steps/s=0.52
[step 1350] loss=0.8636 lr=0.000167 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1457.6MB | elapsed=43.1min, steps/s=0.52
[step 1400] loss=0.8583 lr=0.000165 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1458.3MB | elapsed=44.5min, steps/s=0.52
[step 1450] loss=0.8668 lr=0.000162 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1458.8MB | elapsed=45.8min, steps/s=0.53
[step 1500] loss=0.8557 lr=0.000160 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1459.2MB | elapsed=47.1min, steps/s=0.53
Checkpoint saved at step 1500 → checkpoints/llama32-1b-lora-finetome
[step 1550] loss=0.8881 lr=0.000157 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1460.5MB | elapsed=48.4min, steps/s=0.53
[step 1600] loss=0.8751 lr=0.000155 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1460.9MB | elapsed=49.8min, steps/s=0.54
[step 1650] loss=0.8687 lr=0.000152 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1461.4MB | elapsed=51.1min, steps/s=0.54
[step 1700] loss=0.8303 lr=0.000149 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1461.6MB | elapsed=52.5min, steps/s=0.54
[step 1750] loss=0.8551 lr=0.000146 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1461.9MB | elapsed=53.8min, steps/s=0.54
Checkpoint saved at step 1750 → checkpoints/llama32-1b-lora-finetome
[step 1800] loss=0.8490 lr=0.000143 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1461.9MB | elapsed=55.2min, steps/s=0.54
[step 1850] loss=0.8412 lr=0.000140 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1462.1MB | elapsed=56.5min, steps/s=0.55
[step 1900] loss=0.8339 lr=0.000138 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1462.2MB | elapsed=57.9min, steps/s=0.55
[step 1950] loss=0.8555 lr=0.000135 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1462.3MB | elapsed=59.2min, steps/s=0.55
[step 2000] loss=0.8239 lr=0.000132 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1462.4MB | elapsed=60.6min, steps/s=0.55
Checkpoint saved at step 2000 → checkpoints/llama32-1b-lora-finetome
[step 2050] loss=0.8609 lr=0.000128 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1462.9MB | elapsed=61.8min, steps/s=0.55
[step 2100] loss=0.8543 lr=0.000125 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.0MB | elapsed=63.0min, steps/s=0.56
[step 2150] loss=0.8412 lr=0.000122 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.0MB | elapsed=64.1min, steps/s=0.56
[step 2200] loss=0.8261 lr=0.000119 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.1MB | elapsed=65.3min, steps/s=0.56
[step 2250] loss=0.8653 lr=0.000116 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.1MB | elapsed=66.5min, steps/s=0.56
Checkpoint saved at step 2250 → checkpoints/llama32-1b-lora-finetome
[step 2300] loss=0.8198 lr=0.000113 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.6MB | elapsed=67.7min, steps/s=0.57
[step 2350] loss=0.8512 lr=0.000110 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.6MB | elapsed=68.9min, steps/s=0.57
[step 2400] loss=0.8140 lr=0.000106 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.6MB | elapsed=70.0min, steps/s=0.57
[step 2450] loss=0.8352 lr=0.000103 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.6MB | elapsed=71.2min, steps/s=0.57
[step 2500] loss=0.7731 lr=0.000100 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=1463.7MB | elapsed=72.4min, steps/s=0.58
Checkpoint saved at step 2500 → checkpoints/llama32-1b-lora-finetome
[step 2550] loss=0.7556 lr=0.000097 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.1MB | elapsed=73.6min, steps/s=0.58
[step 2600] loss=0.8091 lr=0.000094 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.1MB | elapsed=74.8min, steps/s=0.58
[step 2650] loss=0.7943 lr=0.000090 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.1MB | elapsed=76.0min, steps/s=0.58
[step 2700] loss=0.7623 lr=0.000087 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.1MB | elapsed=77.2min, steps/s=0.58
[step 2750] loss=0.8028 lr=0.000084 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.1MB | elapsed=78.3min, steps/s=0.59
Checkpoint saved at step 2750 → checkpoints/llama32-1b-lora-finetome
[step 2800] loss=0.7552 lr=0.000081 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.8MB | elapsed=79.5min, steps/s=0.59
[step 2850] loss=0.7642 lr=0.000078 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.8MB | elapsed=80.8min, steps/s=0.59
[step 2900] loss=0.7534 lr=0.000075 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.8MB | elapsed=81.9min, steps/s=0.59
[step 2950] loss=0.7900 lr=0.000072 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.8MB | elapsed=83.1min, steps/s=0.59
[step 3000] loss=0.7645 lr=0.000068 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.8MB | elapsed=84.3min, steps/s=0.59
Checkpoint saved at step 3000 → checkpoints/llama32-1b-lora-finetome
[step 3050] loss=0.7931 lr=0.000065 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.3MB | elapsed=85.5min, steps/s=0.59
[step 3100] loss=0.7765 lr=0.000062 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.3MB | elapsed=86.7min, steps/s=0.60
[step 3150] loss=0.7589 lr=0.000060 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.3MB | elapsed=87.9min, steps/s=0.60
[step 3200] loss=0.7970 lr=0.000057 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.3MB | elapsed=89.1min, steps/s=0.60
[step 3250] loss=0.7448 lr=0.000054 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.3MB | elapsed=90.3min, steps/s=0.60
Checkpoint saved at step 3250 → checkpoints/llama32-1b-lora-finetome
[step 3300] loss=0.7579 lr=0.000051 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=91.5min, steps/s=0.60
[step 3350] loss=0.7253 lr=0.000048 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=92.7min, steps/s=0.60
[step 3400] loss=0.7573 lr=0.000045 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=93.8min, steps/s=0.60
[step 3450] loss=0.7568 lr=0.000043 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=95.0min, steps/s=0.61
[step 3500] loss=0.7613 lr=0.000040 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=96.2min, steps/s=0.61
Checkpoint saved at step 3500 → checkpoints/llama32-1b-lora-finetome
[step 3550] loss=0.7823 lr=0.000038 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=97.4min, steps/s=0.61
[step 3600] loss=0.7384 lr=0.000035 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=98.6min, steps/s=0.61
[step 3650] loss=0.7616 lr=0.000033 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=99.8min, steps/s=0.61
[step 3700] loss=0.7649 lr=0.000030 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=101.0min, steps/s=0.61
[step 3750] loss=0.7491 lr=0.000028 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.2MB | elapsed=102.1min, steps/s=0.61
Checkpoint saved at step 3750 → checkpoints/llama32-1b-lora-finetome
[step 3800] loss=0.7649 lr=0.000026 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.7MB | elapsed=103.3min, steps/s=0.61
[step 3850] loss=0.7851 lr=0.000024 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.7MB | elapsed=104.6min, steps/s=0.61
[step 3900] loss=0.7440 lr=0.000022 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.7MB | elapsed=105.7min, steps/s=0.61
[step 3950] loss=0.7580 lr=0.000020 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.7MB | elapsed=106.9min, steps/s=0.62
[step 4000] loss=0.7759 lr=0.000018 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=407.7MB | elapsed=108.1min, steps/s=0.62
Checkpoint saved at step 4000 → checkpoints/llama32-1b-lora-finetome
[step 4050] loss=0.7663 lr=0.000016 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.6MB | elapsed=109.3min, steps/s=0.62
[step 4100] loss=0.7548 lr=0.000014 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.6MB | elapsed=110.5min, steps/s=0.62
[step 4150] loss=0.7666 lr=0.000013 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.6MB | elapsed=111.7min, steps/s=0.62
[step 4200] loss=0.7485 lr=0.000011 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.7MB | elapsed=112.9min, steps/s=0.62
[step 4250] loss=0.7657 lr=0.000010 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.7MB | elapsed=114.1min, steps/s=0.62
Checkpoint saved at step 4250 → checkpoints/llama32-1b-lora-finetome
[step 4300] loss=0.7732 lr=0.000009 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=409.4MB | elapsed=115.3min, steps/s=0.62
[step 4350] loss=0.7670 lr=0.000007 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=409.4MB | elapsed=116.5min, steps/s=0.62
[step 4400] loss=0.7531 lr=0.000006 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=409.4MB | elapsed=117.7min, steps/s=0.62
[step 4450] loss=0.7462 lr=0.000005 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=409.4MB | elapsed=118.8min, steps/s=0.62
[step 4500] loss=0.7572 lr=0.000004 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=409.4MB | elapsed=120.0min, steps/s=0.62
Checkpoint saved at step 4500 → checkpoints/llama32-1b-lora-finetome
[step 4550] loss=0.7593 lr=0.000003 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=439.1MB | elapsed=121.2min, steps/s=0.63
[step 4600] loss=0.7446 lr=0.000003 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=439.1MB | elapsed=122.4min, steps/s=0.63
[step 4650] loss=0.7756 lr=0.000002 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=439.1MB | elapsed=123.6min, steps/s=0.63
[step 4700] loss=0.7899 lr=0.000001 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=439.1MB | elapsed=124.8min, steps/s=0.63
[step 4750] loss=0.7461 lr=0.000001 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=439.1MB | elapsed=126.0min, steps/s=0.63
Checkpoint saved at step 4750 → checkpoints/llama32-1b-lora-finetome
[step 4800] loss=0.7626 lr=0.000000 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=440.1MB | elapsed=127.2min, steps/s=0.63
[step 4850] loss=0.7391 lr=0.000000 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=440.1MB | elapsed=128.3min, steps/s=0.63
[step 4900] loss=0.8028 lr=0.000000 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=440.1MB | elapsed=129.5min, steps/s=0.63
[step 4950] loss=0.7350 lr=0.000000 | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=440.1MB | elapsed=130.7min, steps/s=0.63
Checkpoint saved at step 4950 → checkpoints/llama32-1b-lora-finetome
[step 4950] loss=N/A lr=N/A | GPU alloc=2503.4MB, reserved=10770.0MB | CPU RAM=408.8MB | elapsed=130.7min, steps/s=0.63
=== Training finished ===
Total steps: 4950
Total wall-clock time: 2.18 hours
