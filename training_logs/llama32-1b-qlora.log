=== Training started ===
Output dir: checkpoints/llama32-1b-qlora-finetome
Batch size: 4
Grad accumulate: 4
Epochs: 2
LR: 0.0002
BF16: True
Optimizer: OptimizerNames.PAGED_ADAMW_8BIT
[step 50] loss=1.3079 lr=0.000200 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1347.7MB | elapsed=1.9min, steps/s=0.44
[step 100] loss=1.0003 lr=0.000200 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1355.0MB | elapsed=4.5min, steps/s=0.37
[step 150] loss=0.9754 lr=0.000200 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1361.4MB | elapsed=7.3min, steps/s=0.34
[step 200] loss=0.9635 lr=0.000200 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1366.6MB | elapsed=8.9min, steps/s=0.38
[step 250] loss=0.9110 lr=0.000199 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1371.9MB | elapsed=10.8min, steps/s=0.39
[step 300] loss=0.9354 lr=0.000199 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1376.1MB | elapsed=12.4min, steps/s=0.40
[step 350] loss=0.9395 lr=0.000198 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1380.3MB | elapsed=14.0min, steps/s=0.42
[step 400] loss=0.9396 lr=0.000197 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1383.9MB | elapsed=15.5min, steps/s=0.43
[step 450] loss=0.9416 lr=0.000197 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1387.2MB | elapsed=17.0min, steps/s=0.44
[step 500] loss=0.9157 lr=0.000196 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1390.4MB | elapsed=18.6min, steps/s=0.45
Checkpoint saved at step 500 → checkpoints/llama32-1b-qlora-finetome
[step 550] loss=0.8873 lr=0.000195 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1412.1MB | elapsed=20.4min, steps/s=0.45
[step 600] loss=0.9185 lr=0.000194 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1414.5MB | elapsed=22.1min, steps/s=0.45
[step 650] loss=0.9142 lr=0.000193 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1416.7MB | elapsed=24.2min, steps/s=0.45
[step 700] loss=0.9429 lr=0.000191 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1418.7MB | elapsed=25.7min, steps/s=0.45
[step 750] loss=0.9088 lr=0.000190 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1420.6MB | elapsed=27.2min, steps/s=0.46
[step 800] loss=0.9448 lr=0.000189 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1422.4MB | elapsed=28.7min, steps/s=0.46
[step 850] loss=0.9109 lr=0.000187 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=1424.1MB | elapsed=30.5min, steps/s=0.46
[step 900] loss=0.9249 lr=0.000186 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=832.4MB | elapsed=33.2min, steps/s=0.45
[step 950] loss=0.9045 lr=0.000184 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=833.8MB | elapsed=34.7min, steps/s=0.46
[step 1000] loss=0.8788 lr=0.000182 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=834.9MB | elapsed=36.2min, steps/s=0.46
Checkpoint saved at step 1000 → checkpoints/llama32-1b-qlora-finetome
[step 1050] loss=0.8573 lr=0.000180 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=865.2MB | elapsed=37.9min, steps/s=0.46
[step 1100] loss=0.9117 lr=0.000178 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=866.2MB | elapsed=39.7min, steps/s=0.46
[step 1150] loss=0.8906 lr=0.000176 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=634.7MB | elapsed=41.4min, steps/s=0.46
[step 1200] loss=0.8754 lr=0.000174 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=635.6MB | elapsed=44.1min, steps/s=0.45
[step 1250] loss=0.8880 lr=0.000172 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=636.3MB | elapsed=45.7min, steps/s=0.46
[step 1300] loss=0.9001 lr=0.000170 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=626.4MB | elapsed=47.4min, steps/s=0.46
[step 1350] loss=0.8809 lr=0.000167 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=627.2MB | elapsed=49.0min, steps/s=0.46
[step 1400] loss=0.8758 lr=0.000165 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=627.9MB | elapsed=50.6min, steps/s=0.46
[step 1450] loss=0.8845 lr=0.000162 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=618.4MB | elapsed=52.3min, steps/s=0.46
[step 1500] loss=0.8745 lr=0.000160 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=618.8MB | elapsed=53.9min, steps/s=0.46
Checkpoint saved at step 1500 → checkpoints/llama32-1b-qlora-finetome
[step 1550] loss=0.9069 lr=0.000157 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=621.9MB | elapsed=55.8min, steps/s=0.46
[step 1600] loss=0.8933 lr=0.000155 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=621.8MB | elapsed=57.5min, steps/s=0.46
[step 1650] loss=0.8859 lr=0.000152 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=622.2MB | elapsed=59.4min, steps/s=0.46
[step 1700] loss=0.8463 lr=0.000149 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=622.5MB | elapsed=61.0min, steps/s=0.46
[step 1750] loss=0.8709 lr=0.000146 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=620.7MB | elapsed=62.7min, steps/s=0.46
[step 1800] loss=0.8655 lr=0.000143 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=620.8MB | elapsed=64.4min, steps/s=0.47
[step 1850] loss=0.8579 lr=0.000140 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=621.0MB | elapsed=66.0min, steps/s=0.47
[step 1900] loss=0.8503 lr=0.000138 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=618.9MB | elapsed=67.5min, steps/s=0.47
[step 1950] loss=0.8726 lr=0.000135 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=619.0MB | elapsed=69.1min, steps/s=0.47
[step 2000] loss=0.8388 lr=0.000132 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=619.1MB | elapsed=70.8min, steps/s=0.47
Checkpoint saved at step 2000 → checkpoints/llama32-1b-qlora-finetome
[step 2050] loss=0.8774 lr=0.000128 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=615.1MB | elapsed=72.5min, steps/s=0.47
[step 2100] loss=0.8710 lr=0.000125 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=615.2MB | elapsed=74.1min, steps/s=0.47
[step 2150] loss=0.8573 lr=0.000122 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=615.2MB | elapsed=75.7min, steps/s=0.47
[step 2200] loss=0.8407 lr=0.000119 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=611.7MB | elapsed=77.3min, steps/s=0.47
[step 2250] loss=0.8814 lr=0.000116 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=611.7MB | elapsed=78.8min, steps/s=0.48
[step 2300] loss=0.8351 lr=0.000113 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=611.7MB | elapsed=80.4min, steps/s=0.48
[step 2350] loss=0.8672 lr=0.000110 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=610.4MB | elapsed=81.8min, steps/s=0.48
[step 2400] loss=0.8309 lr=0.000106 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=610.4MB | elapsed=83.6min, steps/s=0.48
[step 2450] loss=0.8502 lr=0.000103 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=610.4MB | elapsed=85.2min, steps/s=0.48
[step 2500] loss=0.7839 lr=0.000100 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=607.5MB | elapsed=86.8min, steps/s=0.48
Checkpoint saved at step 2500 → checkpoints/llama32-1b-qlora-finetome
[step 2550] loss=0.7644 lr=0.000097 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=88.4min, steps/s=0.48
[step 2600] loss=0.8178 lr=0.000094 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=90.3min, steps/s=0.48
[step 2650] loss=0.8043 lr=0.000090 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.3MB | elapsed=92.0min, steps/s=0.48
[step 2700] loss=0.7718 lr=0.000087 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.3MB | elapsed=93.6min, steps/s=0.48
[step 2750] loss=0.8128 lr=0.000084 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=97.2min, steps/s=0.47
[step 2800] loss=0.7636 lr=0.000081 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=98.7min, steps/s=0.47
[step 2850] loss=0.7729 lr=0.000078 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=100.2min, steps/s=0.47
[step 2900] loss=0.7629 lr=0.000075 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=101.9min, steps/s=0.47
[step 2950] loss=0.7984 lr=0.000072 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=104.4min, steps/s=0.47
[step 3000] loss=0.7739 lr=0.000068 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=578.4MB | elapsed=106.1min, steps/s=0.47
Checkpoint saved at step 3000 → checkpoints/llama32-1b-qlora-finetome
[step 3050] loss=0.8012 lr=0.000065 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.8MB | elapsed=107.6min, steps/s=0.47
[step 3100] loss=0.7858 lr=0.000062 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.8MB | elapsed=109.2min, steps/s=0.47
[step 3150] loss=0.7679 lr=0.000060 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.8MB | elapsed=110.7min, steps/s=0.47
[step 3200] loss=0.8056 lr=0.000057 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.7MB | elapsed=112.2min, steps/s=0.48
[step 3250] loss=0.7541 lr=0.000054 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.7MB | elapsed=113.7min, steps/s=0.48
[step 3300] loss=0.7666 lr=0.000051 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.7MB | elapsed=115.2min, steps/s=0.48
[step 3350] loss=0.7341 lr=0.000048 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.4MB | elapsed=116.8min, steps/s=0.48
[step 3400] loss=0.7653 lr=0.000045 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.4MB | elapsed=118.3min, steps/s=0.48
[step 3450] loss=0.7637 lr=0.000043 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.5MB | elapsed=119.9min, steps/s=0.48
[step 3500] loss=0.7691 lr=0.000040 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.5MB | elapsed=121.6min, steps/s=0.48
Checkpoint saved at step 3500 → checkpoints/llama32-1b-qlora-finetome
[step 3550] loss=0.7913 lr=0.000038 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=123.3min, steps/s=0.48
[step 3600] loss=0.7478 lr=0.000035 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=577.0MB | elapsed=124.8min, steps/s=0.48
[step 3650] loss=0.7705 lr=0.000033 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=577.0MB | elapsed=126.5min, steps/s=0.48
[step 3700] loss=0.7741 lr=0.000030 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=128.1min, steps/s=0.48
[step 3750] loss=0.7561 lr=0.000028 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=129.8min, steps/s=0.48
[step 3800] loss=0.7725 lr=0.000026 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=131.4min, steps/s=0.48
[step 3850] loss=0.7937 lr=0.000024 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=133.1min, steps/s=0.48
[step 3900] loss=0.7512 lr=0.000022 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=134.7min, steps/s=0.48
[step 3950] loss=0.7680 lr=0.000020 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=136.4min, steps/s=0.48
[step 4000] loss=0.7848 lr=0.000018 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.9MB | elapsed=137.9min, steps/s=0.48
Checkpoint saved at step 4000 → checkpoints/llama32-1b-qlora-finetome
[step 4050] loss=0.7761 lr=0.000016 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.1MB | elapsed=139.7min, steps/s=0.48
[step 4100] loss=0.7626 lr=0.000014 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.1MB | elapsed=141.3min, steps/s=0.48
[step 4150] loss=0.7739 lr=0.000013 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.0MB | elapsed=142.9min, steps/s=0.48
[step 4200] loss=0.7563 lr=0.000011 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.0MB | elapsed=144.5min, steps/s=0.48
[step 4250] loss=0.7742 lr=0.000010 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.0MB | elapsed=146.1min, steps/s=0.48
[step 4300] loss=0.7818 lr=0.000009 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.0MB | elapsed=148.0min, steps/s=0.48
[step 4350] loss=0.7751 lr=0.000007 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.0MB | elapsed=149.6min, steps/s=0.48
[step 4400] loss=0.7603 lr=0.000006 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=606.1MB | elapsed=151.2min, steps/s=0.49
[step 4450] loss=0.7543 lr=0.000005 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=605.7MB | elapsed=152.9min, steps/s=0.49
[step 4500] loss=0.7659 lr=0.000004 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=605.7MB | elapsed=154.4min, steps/s=0.49
Checkpoint saved at step 4500 → checkpoints/llama32-1b-qlora-finetome
[step 4550] loss=0.7671 lr=0.000003 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=576.1MB | elapsed=156.0min, steps/s=0.49
[step 4600] loss=0.7534 lr=0.000003 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=157.7min, steps/s=0.49
[step 4650] loss=0.7848 lr=0.000002 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=159.4min, steps/s=0.49
[step 4700] loss=0.7985 lr=0.000001 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=160.9min, steps/s=0.49
[step 4750] loss=0.7531 lr=0.000001 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=162.4min, steps/s=0.49
[step 4800] loss=0.7704 lr=0.000000 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=163.9min, steps/s=0.49
[step 4850] loss=0.7470 lr=0.000000 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=165.4min, steps/s=0.49
[step 4900] loss=0.8113 lr=0.000000 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.2MB | elapsed=167.0min, steps/s=0.49
[step 4950] loss=0.7441 lr=0.000000 | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=574.5MB | elapsed=168.6min, steps/s=0.49
Checkpoint saved at step 4950 → checkpoints/llama32-1b-qlora-finetome
[step 4950] loss=N/A lr=N/A | GPU alloc=1050.4MB, reserved=10886.0MB | CPU RAM=575.7MB | elapsed=168.6min, steps/s=0.49
=== Training finished ===
Total steps: 4950
Total wall-clock time: 2.81 hours
